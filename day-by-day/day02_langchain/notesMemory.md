# ğŸ“˜ Day 2 â€“ RAG Chain with Memory (Ollama + LLaMA3)

> **Goal:** Extend the refactored RAG pipeline to support **multi-turn conversational memory** using LangChain's `ConversationalRetrievalChain`.

---

## ğŸ§  What I Did

- Built on my modular RAG pipeline from earlier
- Introduced **LangChain memory** via `ConversationBufferMemory`
- Switched to `ConversationalRetrievalChain` to handle:
  - Chat history (multi-turn)
  - RAG-based document retrieval
- Used **LLaMA3 via Ollama** to run everything locally
- Debugged LangChainâ€™s strict requirements for memory â†’ fixed by setting both `output_key` values correctly

---

## ğŸ§© Tech Stack

| Component | Tool |
|----------|------|
| LLM | `ChatOllama(model="llama3")` |
| Memory | `ConversationBufferMemory` |
| Chain | `ConversationalRetrievalChain` |
| Retrieval | `Chroma + HuggingFaceEmbeddings` |
| Docs | `PyPDFLoader` from `langchain_community` |

---

## ğŸ§  Key Implementation Notes

### âœ… Memory and Chain Setup
```python
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"  # ğŸ› ï¸ critical to avoid error
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True,
    output_key="answer"  # ğŸ› ï¸ required for multi-key return
)

while True:
    query = input("ğŸ§  You: ")
    if query.lower() in ["exit", "quit"]:
        break

    result = qa_chain.invoke({"question": query})
    print("ğŸ¤–", result["answer"])
```
