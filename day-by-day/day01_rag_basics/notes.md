# ğŸ“˜ Day 1 â€“ RAG Systems 101

> **Goal:** Understand and implement a working prototype of Retrieval-Augmented Generation (RAG) using LangChain, ChromaDB, and Hugging Face embeddings.

---

## ğŸ§  What is RAG?

**RAG (Retrieval-Augmented Generation)** is a method that enhances the capabilities of LLMs by allowing them to retrieve external knowledge from a document store to generate grounded, factual answers.

### âš™ï¸ RAG Pipeline Components:
1. **Document Ingestion** â€“ Load structured/unstructured data (e.g., PDFs).
2. **Text Chunking** â€“ Split documents into digestible pieces (chunks).
3. **Embeddings** â€“ Convert text chunks into vector representations.
4. **Vector Store** â€“ Store and retrieve chunks based on semantic similarity.
5. **Retriever + Prompt + LLM** â€“ Compose a chain to:
   - Retrieve relevant chunks
   - Format a prompt with context
   - Generate an accurate answer

---

## ğŸ› ï¸ Tools & Libraries Used

| Tool | Purpose |
|------|---------|
| **LangChain** | Build modular LLM chains |
| **ChromaDB** | Fast and local vector database |
| **Hugging Face Embeddings** | Generate dense vector representations |
| **PyPDFLoader** | Parse content from PDF documents |
| **PromptTemplate** | Define input structure for LLMs |
| **RunnablePassthrough / OutputParser** | Control flow and parse LLM output |

---

## ğŸ“„ Document Used

We loaded a real-world medical document (PDF) that could represent internal Novartis materials like:
- **GMA Strategy Documents**
- **Neato Internal Guides**
- **Scientific Literature**

---

## ğŸ’¡ Highlights from the Implementation

### âœ… Loading and Chunking
```python
loader = PyPDFLoader("TS3043166.pdf")
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
```

### âœ… Embedding and Indexing
```python
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

### âœ… Defining the Prompt
```python
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Use the following context to answer the question. Be concise and specific.

Context:
{context}

Question:
{question}
"""
)
```

### âœ… Running the RAG Chain
```python
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("What is CLALIT?")
```
